{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"/content/drive/MyDrive/NER_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representation of lists to actual lists\n",
    "df['Word'] = df['Word'].apply(literal_eval)\n",
    "df['POS'] = df['POS'].apply(literal_eval)\n",
    "df['Tag'] = df['Tag'].apply(literal_eval)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_from_df = []\n",
    "for index, row in df.head(10000).iterrows():\n",
    "  text = \"\"\n",
    "  n = len(row['Word'])\n",
    "  for i in range(0, n-1):\n",
    "    text = text + row['Word'][i] + \" \"\n",
    "  text = text + row['Word'][-1]\n",
    "  # print(f\"Length of Text {i} : {n}. Length of POS {i} : {len(row['POS'])}\")\n",
    "  text_from_df.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(f\"Text {i} : {text_from_df[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.head(10).iterrows():\n",
    "  print(f\"{row['POS']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER Tagging Using SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_suffix_regex, compile_infix_regex\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tag mapping\n",
    "tag_mapping = {\n",
    "    \"B-art\": \"ART\",\n",
    "    \"B-eve\": \"EVENT\",\n",
    "    \"B-geo\": \"GPE\",\n",
    "    \"B-gpe\": \"GPE\",\n",
    "    \"B-nat\": \"NORP\",\n",
    "    \"B-org\": \"ORG\",\n",
    "    \"B-per\": \"PERSON\",\n",
    "    \"B-tim\": \"TIME\",\n",
    "    \"I-art\": \"ART\",\n",
    "    \"I-eve\": \"EVENT\",\n",
    "    \"I-geo\": \"GPE\",\n",
    "    \"I-gpe\": \"GPE\",\n",
    "    \"I-nat\": \"NORP\",\n",
    "    \"I-org\": \"ORG\",\n",
    "    \"I-per\": \"PERSON\",\n",
    "    \"I-tim\": \"TIME\",\n",
    "    \"O\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to preprocess words\n",
    "def preprocess_words(words):\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        # Replace hyphens in words with an empty string\n",
    "        processed_word = re.sub(r'[-~\\']', '', word)\n",
    "        processed_words.append(processed_word)\n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    # Preprocess words before joining\n",
    "    processed_words = preprocess_words(df['Word'][i])\n",
    "    text = ' '.join(processed_words)\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    if len(df['Word'][i]) == len(pos_tags):\n",
    "        displacy.render(doc, style=\"dep\", jupyter=True, options={'distance':100})\n",
    "        # Evaluate NER Tags\n",
    "        displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "n = len(df)\n",
    "\n",
    "# Process each sentence and evaluate POS and NER\n",
    "pos_predictions = []\n",
    "ner_predictions = []\n",
    "pos_true = []\n",
    "ner_true = []\n",
    "\n",
    "for i in range(n):\n",
    "    # Preprocess words before joining\n",
    "    processed_words = preprocess_words(df['Word'][i])\n",
    "    text = ' '.join(processed_words)\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    if len(df['Word'][i]) == len(pos_tags):\n",
    "        pos_predictions.extend(pos_tags)\n",
    "        pos_true.extend(df['POS'][i])\n",
    "        # Evaluate NER Tags\n",
    "        ner_tags = [alpha.ent_type_ for alpha in doc]\n",
    "        ner_predictions.extend(ner_tags)\n",
    "        ner_true.extend(df['Tag'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tags using the dictionary\n",
    "mapped_ner_true = [tag_mapping[tag] for tag in ner_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print POS classification report\n",
    "pos_report = classification_report(pos_true, pos_predictions)\n",
    "print(\"POS Tagging Classification Report:\")\n",
    "print(pos_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print Tag classification report\n",
    "tag_report = classification_report(mapped_ner_true, ner_predictions)\n",
    "print(\"Tag Classification Report:\")\n",
    "print(tag_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_pos = accuracy_score(pos_true, pos_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_pos = f1_score(pos_true, pos_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_tag = accuracy_score(mapped_ner_true, ner_predictions)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1_tag = f1_score(mapped_ner_true, ner_predictions, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of Spacy model on pos is {accuracy_pos}\")\n",
    "print(f\"F1 Score of Spacy model on pos is {f1_pos}\")\n",
    "\n",
    "print(f\"Accuracy of Spacy model on tag is {accuracy_tag}\")\n",
    "print(f\"F1 Score of Spacy model on tag is {f1_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf_matrix_pos = confusion_matrix(pos_true, pos_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for Spacy model on pos is :\")\n",
    "print(conf_matrix_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf_matrix_tag = confusion_matrix(mapped_ner_true, ner_predictions)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for Spacy model on tag is :\")\n",
    "print(conf_matrix_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER Tagging Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original to NLTK Tags Mapping\n",
    "tag_mapping_nltk = {\n",
    "    \"B-art\": \"O\",  # Assuming no direct equivalent for B-art in NLTK tags\n",
    "    \"B-eve\": \"O\",  # Assuming no direct equivalent for B-eve in NLTK tags\n",
    "    \"B-geo\": \"LOCATION\",\n",
    "    \"B-gpe\": \"GPE\",\n",
    "    \"B-nat\": \"O\",  # Assuming no direct equivalent for B-nat in NLTK tags\n",
    "    \"B-org\": \"ORGANIZATION\",\n",
    "    \"B-per\": \"PERSON\",\n",
    "    \"B-tim\": \"TIME\",\n",
    "    \"I-art\": \"O\",  # Assuming no direct equivalent for I-art in NLTK tags\n",
    "    \"I-eve\": \"O\",  # Assuming no direct equivalent for I-eve in NLTK tags\n",
    "    \"I-geo\": \"LOCATION\",\n",
    "    \"I-gpe\": \"GPE\",\n",
    "    \"I-nat\": \"O\",  # Assuming no direct equivalent for I-nat in NLTK tags\n",
    "    \"I-org\": \"ORGANIZATION\",\n",
    "    \"I-per\": \"PERSON\",\n",
    "    \"I-tim\": \"TIME\",\n",
    "    \"O\": \"O\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "n = 10\n",
    "\n",
    "# Function to convert NLTK tree structure to flat list\n",
    "def extract_ner_tags(tree):\n",
    "    ner_tags = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == Tree:\n",
    "            ner_tags.append(f\"{subtree.label()}-{subtree[0][0]}\")\n",
    "        else:\n",
    "            ner_tags.append(\"O\")  # Outside named entities\n",
    "    return ner_tags\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    # Preprocess words before joining\n",
    "    processed_words = preprocess_words(df['Word'][i])\n",
    "    text = ' '.join(processed_words)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Predict POS tags\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    if len(df['Word'][i]) == len(pos_tags):\n",
    "        print(pos_tags)\n",
    "\n",
    "        # Predict NER tags using NLTK's ne_chunk\n",
    "        chunked = ne_chunk(pos_tags)\n",
    "        ner_tags = extract_ner_tags(chunked)\n",
    "        print(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "n = len(df)\n",
    "\n",
    "# Function to convert NLTK tree structure to flat list\n",
    "def extract_ner_tags(tree):\n",
    "    ner_tags = []\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == Tree:\n",
    "            ner_tags.append(f\"{subtree.label()}-{subtree[0][1]}\")\n",
    "        else:\n",
    "            ner_tags.append(\"O\")  # Outside named entities\n",
    "    return ner_tags\n",
    "\n",
    "# Initialize lists to store the true and predicted POS and NER tags\n",
    "pos_predictions_nltk = []\n",
    "ner_predictions_nltk = []\n",
    "pos_true_nltk = []\n",
    "ner_true_nltk = []\n",
    "\n",
    "for i in range(n):\n",
    "    # Preprocess words before joining\n",
    "    processed_words = preprocess_words(df['Word'][i])\n",
    "    text = ' '.join(processed_words)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Predict POS tags\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    if len(df['Word'][i]) == len(pos_tags):\n",
    "        pos_predictions_nltk.extend([tag for _, tag in pos_tags])\n",
    "        pos_true_nltk.extend(df['POS'][i])\n",
    "\n",
    "        # Predict NER tags using NLTK's ne_chunk\n",
    "        chunked = ne_chunk(pos_tags)\n",
    "        ner_tags = extract_ner_tags(chunked)\n",
    "\n",
    "        # Flatten the nested structure\n",
    "        ner_tags_flat = [tag if \"-\" not in tag else tag.split(\"-\")[0] for tag in ner_tags]\n",
    "        if len(ner_tags_flat) == len(df['Tag'][i]):\n",
    "            ner_predictions_nltk.extend(ner_tags_flat)\n",
    "            ner_true_nltk.extend(df['Tag'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tags using the dictionary\n",
    "mapped_ner_true_nltk = [tag_mapping_nltk[tag] for tag in ner_true_nltk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print POS classification report\n",
    "pos_report_nltk = classification_report(pos_true_nltk, pos_predictions_nltk)\n",
    "print(\"POS Tagging Classification Report for nltk:\")\n",
    "print(pos_report_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and print Tag classification report\n",
    "tag_report_nltk = classification_report(mapped_ner_true_nltk, ner_predictions_nltk)\n",
    "print(\"Tag Classification Report for nltk:\")\n",
    "print(tag_report_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy_pos_nltk = accuracy_score(pos_true_nltk, pos_predictions_nltk)\n",
    "# Calculate F1 score\n",
    "f1_pos_nltk = f1_score(pos_true_nltk, pos_predictions_nltk, average='weighted')\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_tag_nltk = accuracy_score(mapped_ner_true_nltk, ner_predictions_nltk)\n",
    "# Calculate F1 score\n",
    "f1_tag_nltk = f1_score(mapped_ner_true_nltk, ner_predictions_nltk, average='weighted')\n",
    "\n",
    "print(f\"Accuracy of NLTK model on POS is {accuracy_pos_nltk}\")\n",
    "print(f\"F1 Score of NLTK model on POS is {f1_pos_nltk}\")\n",
    "print(f\"Accuracy of NLTK model on tag is {accuracy_tag_nltk}\")\n",
    "print(f\"F1 Score of NLTK model on tag is {f1_tag_nltk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf_matrix_pos_nltk = confusion_matrix(pos_true_nltk, pos_predictions_nltk)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for nltk model on pos is :\")\n",
    "print(conf_matrix_pos_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "conf_matrix_tag_nltk = confusion_matrix(mapped_ner_true_nltk, ner_predictions_nltk)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix for nltk model on tag is :\")\n",
    "print(conf_matrix_tag_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and formatting dataset(for sentiment analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import random\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import random\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# Upload the gzip compressed JSON files\n",
    "uploaded_files = files.upload()\n",
    "\n",
    "# List of file names\n",
    "file_names = list(uploaded_files.keys())\n",
    "\n",
    "# Function to load and process the data\n",
    "def load_and_process_data(file_name):\n",
    "    with gzip.open(file_name, 'rt') as file:\n",
    "        reviews = [json.loads(line) for line in file]\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Load and process each file\n",
    "datasets = []\n",
    "for file_name in file_names:\n",
    "    dataset = load_and_process_data(file_name)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_dataset = [review for dataset in datasets for review in dataset]\n",
    "\n",
    "# Create a DataFrame from the combined dataset\n",
    "df = pd.DataFrame(combined_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map overall ratings to binary classes\n",
    "df['sentiment'] = df['overall'].apply(lambda x: 'negative' if x <= 2 else 'positive')\n",
    "\n",
    "# Select only the desired columns\n",
    "df_1 = df[['reviewText', 'summary', 'overall']]\n",
    "\n",
    "# Display the modified DataFrame\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'reviewText' or 'summary'\n",
    "df_2 = df_1.dropna()\n",
    "df_2.to_csv('df_2.csv', index=False)\n",
    "import pandas as pd\n",
    "df_2=pd.read_csv(\"/content/drive/MyDrive/df_2.csv\")\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_2[['reviewText', 'summary']]\n",
    "y = df_2['overall']\n",
    "\n",
    "# Define the train-val split with stratification\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isnull(text):  # Check if the text is NaN\n",
    "        return ''\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and apply stemming\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [porter.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to both reviewText and summary\n",
    "X_train['reviewText'] = X_train['reviewText'].apply(preprocess_text)\n",
    "X_train['summary'] = X_train['summary'].apply(preprocess_text)\n",
    "\n",
    "X_val['reviewText'] = X_val['reviewText'].apply(preprocess_text)\n",
    "X_val['summary'] = X_val['summary'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dr = X_train.drop('summary', axis=1)\n",
    "X_val_dr = X_val.drop('summary', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_counts = df_2['overall'].value_counts()\n",
    "\n",
    "# Plotting the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribution of Classes in df_2')\n",
    "plt.xlabel('Overall Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'overall' to binary (1 for positive, 0 for negative)\n",
    "df_2['sentiment'] = df_2['overall'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "# Count the occurrences of each class in the 'sentiment' column\n",
    "sentiment_counts = df_2['sentiment'].value_counts()\n",
    "\n",
    "# Plotting the bar plot for binary classification\n",
    "plt.figure(figsize=(6, 4))\n",
    "sentiment_counts.sort_index().plot(kind='bar', color='salmon')\n",
    "plt.title('Distribution of Sentiments in df_2 (Binary)')\n",
    "plt.xlabel('Sentiment (0: Negative, 1: Positive)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Review Sentiment Analysis using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "def evaluate_performance(y_true, y_pred, classifier_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    classification_rep = classification_report(y_true, y_pred)\n",
    "    F1_score = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Performance for {classifier_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1_score: {F1_score}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train['reviewText'] + ' ' + X_train['summary'])\n",
    "X_val_bow = vectorizer.transform(X_val['reviewText'] + ' ' + X_val['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "\n",
    "X_train_bow_dr = vectorizer.fit_transform(X_train['reviewText'])\n",
    "X_val_bow_dr = vectorizer.transform(X_val['reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "nb_classifier = MultinomialNB()\n",
    "gnb_classifier = GaussianNB()\n",
    "dt_classifier_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "dt_classifier_gini = DecisionTreeClassifier(criterion='gini')\n",
    "rf_classifier_20 = RandomForestClassifier(n_estimators=20)\n",
    "rf_classifier_50 = RandomForestClassifier(n_estimators=50)\n",
    "rf_classifier_100 = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "nb_classifier.fit(X_train_bow, y_train)\n",
    "gnb_classifier.fit(X_train_bow.toarray(), y_train)  # GaussianNB expects dense matrix\n",
    "dt_classifier_entropy.fit(X_train_bow, y_train)\n",
    "dt_classifier_gini.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "nb_preds = nb_classifier.predict(X_val_bow)\n",
    "gnb_preds = gnb_classifier.predict(X_val_bow.toarray())\n",
    "dt_preds_entropy = dt_classifier_entropy.predict(X_val_bow)\n",
    "dt_preds_gini = dt_classifier_gini.predict(X_val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, nb_preds, 'Naive Bayes')\n",
    "evaluate_performance(y_val, gnb_preds, 'Gaussian Naive Bayes')\n",
    "evaluate_performance(y_val, dt_preds_entropy, 'Decision Tree (Entropy)')\n",
    "evaluate_performance(y_val, dt_preds_gini, 'Decision Tree (Gini)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_20.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_preds_20 = rf_classifier_20.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_20, 'Random Forest (20 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "rf_classifier_50_dr = RandomForestClassifier(n_estimators=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_50.fit(X_train_bow, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_classifier_50, 'rf_classifier_50.pkl')\n",
    "\n",
    "# Predictions\n",
    "rf_preds_50 = rf_classifier_50.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_50, 'Random Forest (50 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_50_dr.fit(X_train_bow_dr, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(rf_classifier_50_dr, 'rf_classifier_50_dr.pkl')\n",
    "\n",
    "# Predictions\n",
    "rf_preds_50_dr = rf_classifier_50_dr.predict(X_val_bow_dr)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_50_dr, 'Random Forest (50 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load('rf_classifier_50_dr.pkl')\n",
    "def generate_predictions(test_file):\n",
    "    # Load the test data\n",
    "    test_df = pd.read_csv(test_file)\n",
    "\n",
    "    test = test_df.copy()\n",
    "\n",
    "    # Preprocess the text data\n",
    "    test['review_text'] = test['review_text'].apply(preprocess_text)\n",
    "\n",
    "    # Assuming X_test is the feature column where the reviews are stored\n",
    "    # X_test = test\n",
    "    X_test = vectorizer.transform(test['review_text'])\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = loaded_model.predict(X_test)\n",
    "\n",
    "    # Add predictions to the DataFrame\n",
    "    test_df['result'] = predictions\n",
    "\n",
    "    # Save the DataFrame to a new CSV file\n",
    "    output_filename = 'Group_17_cv.csv'  # Replace {Group ID} with your group ID\n",
    "    test_df.to_csv(output_filename, index=False, columns=['review_text', 'result'])\n",
    "\n",
    "# Replace 'test_file.csv' with the path to your test CSV file\n",
    "generate_predictions('Group_17.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_df = pd.read_csv('Group_17.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "test_df['review'] = test_df['review'].apply(preprocess_text)\n",
    "\n",
    "# Assuming X_test is the feature column where the reviews are stored\n",
    "X_test = test_df\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_100.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_preds_100 = rf_classifier_100.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_100, 'Random Forest (100 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, nb_preds)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, gnb_preds)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Gaussian Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, dt_preds_entropy)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Decision Tree Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, dt_preds_gini)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Decision Tree Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_20)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Random Forest 20')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_50)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Random Forest 50')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_100)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Multiclass Classification Random Forest 100')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "def evaluate_performance(y_true, y_pred, classifier_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    classification_rep = classification_report(y_true, y_pred)\n",
    "    F1_score = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Performance for {classifier_name}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1_score: {F1_score}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train-val split with stratification\n",
    "X = df_2[['reviewText', 'summary']]\n",
    "y = (df_2['overall'] > 2).astype(int)  # 1 for positive (3, 4, 5), 0 for negative (1, 2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train['reviewText'] + ' ' + X_train['summary'])\n",
    "X_val_bow = vectorizer.transform(X_val['reviewText'] + ' ' + X_val['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "nb_classifier = MultinomialNB()\n",
    "gnb_classifier = GaussianNB()\n",
    "dt_classifier_entropy = DecisionTreeClassifier(criterion='entropy')\n",
    "dt_classifier_gini = DecisionTreeClassifier(criterion='gini')\n",
    "rf_classifier_20 = RandomForestClassifier(n_estimators=20)\n",
    "rf_classifier_50 = RandomForestClassifier(n_estimators=50)\n",
    "rf_classifier_100 = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "nb_classifier.fit(X_train_bow, y_train)\n",
    "dt_classifier_entropy.fit(X_train_bow, y_train)\n",
    "dt_classifier_gini.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "nb_preds = nb_classifier.predict(X_val_bow)\n",
    "dt_preds_entropy = dt_classifier_entropy.predict(X_val_bow)\n",
    "dt_preds_gini = dt_classifier_gini.predict(X_val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, nb_preds, 'Naive Bayes')\n",
    "evaluate_performance(y_val, dt_preds_entropy, 'Decision Tree (Entropy)')\n",
    "evaluate_performance(y_val, dt_preds_gini, 'Decision Tree (Gini)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_20.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_preds_20 = rf_classifier_20.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_20, 'Random Forest (20 trees)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_50.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_preds_50 = rf_classifier_50.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_50, 'Random Forest (50 trees)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers\n",
    "rf_classifier_100.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_preds_100 = rf_classifier_100.predict(X_val_bow)\n",
    "\n",
    "# Evaluate each classifier\n",
    "evaluate_performance(y_val, rf_preds_100, 'Random Forest (100 trees)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, nb_preds)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Naive Bayes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, dt_preds_entropy)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Decision Tree Entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, dt_preds_gini)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Decision Tree Gini')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_20)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Random Forest 20')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_50)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Random Forest 50')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(y_val, rf_preds_100)\n",
    "\n",
    "# Display confusion matrix with labels\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Binary Classification Random Forest 100')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
